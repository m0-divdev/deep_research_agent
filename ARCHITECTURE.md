# Deep Research Multi-Agent System Architecture

## ğŸ—ï¸ System Overview

This system implements a sophisticated multi-agent research architecture where four specialized AI agents collaborate through a coordinated workflow to perform comprehensive research tasks. Each agent directly integrates with Parallel.ai APIs and uses the Agno framework for intelligent reasoning.

## ğŸ¯ Core Philosophy

- **Specialized Agents**: Each agent has a specific role and expertise
- **Sequential Workflow**: Agents work in a dependency-based pipeline
- **Direct API Integration**: No middleware - agents directly call Parallel.ai APIs
- **Shared Intelligence**: All agents share knowledge through a central repository
- **Fault Tolerance**: Robust error handling and workflow management

## ğŸ“Š Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          ğŸ›ï¸ AGENT ORCHESTRATOR                              â”‚
â”‚                     (System Controller & API Gateway)                       â”‚
â”‚  â€¢ Manages agent lifecycle    â€¢ Handles API requests                       â”‚
â”‚  â€¢ Coordinates workflows      â€¢ Provides system status                     â”‚
â”‚  â€¢ Background task processing â€¢ Knowledge management                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚                     â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
         â”‚                         â”‚       â”‚
         â–¼                         â–¼       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ”„ WORKFLOW     â”‚    â”‚  ğŸ“‹ TASK        â”‚ â”‚  ğŸ§  SHARED      â”‚
â”‚     ENGINE       â”‚    â”‚  COORDINATOR    â”‚ â”‚  KNOWLEDGE      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ â€¢ Step execution â”‚    â”‚ â€¢ Queue mgmt    â”‚ â”‚ â€¢ Central store â”‚
â”‚ â€¢ Dependency mgmtâ”‚    â”‚ â€¢ Task routing  â”‚ â”‚ â€¢ Cross-agent   â”‚
â”‚ â€¢ Error handling â”‚    â”‚ â€¢ Status track  â”‚ â”‚   communication â”‚
â”‚ â€¢ Parallel tasks â”‚    â”‚ â€¢ History logs  â”‚ â”‚ â€¢ Search index  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                         â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                  â”‚                  â”‚
    â–¼                  â–¼                  â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ğŸ” SEARCHâ”‚    â”‚ğŸ“Š ANALYST   â”‚    â”‚âœ… CRITIC    â”‚    â”‚âœï¸ WRITER    â”‚
â”‚  AGENT  â”‚â”€â”€â”€â”€â”‚   AGENT     â”‚â”€â”€â”€â”€â”‚   AGENT     â”‚â”€â”€â”€â”€â”‚   AGENT     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                â”‚                  â”‚                  â”‚
     â”‚                â”‚                  â”‚                  â”‚
     â–¼                â–¼                  â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Parallel â”‚    â”‚  Parallel   â”‚    â”‚  Parallel   â”‚    â”‚  Parallel   â”‚
â”‚.ai Task â”‚    â”‚  .ai Task   â”‚    â”‚  .ai Task   â”‚    â”‚  .ai Task   â”‚
â”‚API      â”‚    â”‚  API        â”‚    â”‚  API        â”‚    â”‚  API        â”‚
â”‚(Search) â”‚    â”‚ (Analysis)  â”‚    â”‚(Verification)â”‚    â”‚(Generation) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ Research Workflow Pipeline

```
ğŸ“ USER QUERY
     â”‚
     â–¼
ğŸ” STEP 1: SEARCH AGENT
â”‚  â”œâ”€ Web research via Parallel.ai Task API
â”‚  â”œâ”€ Information gathering and extraction  
â”‚  â”œâ”€ Source identification and ranking
â”‚  â””â”€ Raw data collection
     â”‚
     â–¼
ğŸ“Š STEP 2: ANALYST AGENT  
â”‚  â”œâ”€ Data processing and analysis
â”‚  â”œâ”€ Pattern recognition and insights
â”‚  â”œâ”€ Statistical analysis and trends
â”‚  â””â”€ Structured findings generation
     â”‚
     â–¼
âœ… STEP 3: CRITIC AGENT
â”‚  â”œâ”€ Fact-checking and verification
â”‚  â”œâ”€ Source credibility assessment
â”‚  â”œâ”€ Bias detection and correction
â”‚  â””â”€ Quality assurance validation
     â”‚
     â–¼
âœï¸ STEP 4: WRITER AGENT
â”‚  â”œâ”€ Content synthesis and generation
â”‚  â”œâ”€ Report structuring and formatting
â”‚  â”œâ”€ Citation management
â”‚  â””â”€ Final deliverable creation
     â”‚
     â–¼
ğŸ“„ RESEARCH REPORT
```

## ğŸ¤– Agent Detailed Specifications

### ğŸ” Search Agent - Information Retrieval Specialist
**Role**: First-stage data collection and web research
- **Framework**: Agno Agent + AsyncParallel client
- **API Method**: `client.task_run.execute()`
- **Processors**: `base` (fast), `pro` (comprehensive)
- **Capabilities**:
  - Web search and information gathering
  - Source identification and ranking
  - Data extraction and preprocessing
  - Query optimization and refinement
- **Input**: Natural language research query
- **Output**: Structured search results with sources
- **Memory**: Stores search history, results, and API responses

### ğŸ“Š Analyst Agent - Data Processing Specialist  
**Role**: Transform raw data into meaningful insights
- **Framework**: Agno Agent + AsyncParallel client
- **API Method**: `client.task_run.execute()`
- **Processors**: `core` (standard), `ultra` (complex analysis)
- **Capabilities**:
  - Statistical analysis and pattern recognition
  - Data correlation and trend identification
  - Insight generation and hypothesis formation
  - Structured data transformation
- **Input**: Raw search results from Search Agent
- **Output**: Processed insights, findings, and analysis
- **Memory**: Stores analytical results, insights, and processed data

### âœ… Critic Agent - Verification & Quality Assurance Specialist
**Role**: Fact-checking, validation, and quality control
- **Framework**: Agno Agent + AsyncParallel client  
- **API Method**: `client.task_run.execute()`
- **Processors**: `pro` (fact-checking), `ultra` (complex validation)
- **Capabilities**:
  - Fact-checking and source verification
  - Bias detection and credibility assessment
  - Cross-referencing and validation
  - Quality scoring and confidence metrics
- **Input**: Analysis results from Analyst Agent
- **Output**: Verified data with confidence scores and evidence
- **Memory**: Stores verification results, fact-checks, and validation data

### âœï¸ Writer Agent - Content Generation Specialist
**Role**: Synthesize verified data into comprehensive reports
- **Framework**: Agno Agent + AsyncParallel client
- **API Method**: `client.task_run.execute()`  
- **Processors**: `lite` (fast generation), `base` (quality content)
- **Capabilities**:
  - Content synthesis and report generation
  - Template-based formatting and structuring
  - Citation management and source attribution
  - Multi-format output (markdown, HTML, plain text)
- **Input**: Verified data from Critic Agent
- **Output**: Comprehensive research reports with proper citations
- **Memory**: Stores generated content, templates, and formatting data

## ğŸ§  Memory & Knowledge Management System

### Specialized Memory Components
Each agent has dedicated memory storage optimized for its specific data types:

#### ğŸ” Search Memory
- **Purpose**: Stores web research data and search metadata
- **Contents**: 
  - Search queries and results
  - API response data and timestamps
  - Source URLs and credibility scores
  - Search history and query optimization data
- **Access Pattern**: Read-heavy for cross-referencing and duplicate detection

#### ğŸ“Š Analysis Memory  
- **Purpose**: Stores processed insights and analytical results
- **Contents**:
  - Statistical analysis results and confidence scores
  - Pattern recognition data and trend analysis
  - Insights, hypotheses, and structured findings
  - Data correlation matrices and relationships
- **Access Pattern**: Write-heavy during analysis, read-heavy for verification

#### âœ… Verification Memory
- **Purpose**: Stores fact-checking and validation results  
- **Contents**:
  - Fact-check results with evidence and sources
  - Credibility assessments and bias detection
  - Cross-reference validation data
  - Quality scores and confidence metrics
- **Access Pattern**: Sequential write during verification, random read for reporting

#### âœï¸ Content Memory
- **Purpose**: Stores generated content and formatting templates
- **Contents**:
  - Generated reports and content templates
  - Formatting rules and style guidelines  
  - Citation databases and source attributions
  - Multi-format output versions (MD, HTML, PDF)
- **Access Pattern**: Template-based read, versioned write for content generation

#### ğŸŒ Shared Knowledge Repository
- **Purpose**: Central hub for cross-agent communication and knowledge sharing
- **Contents**:
  - Unified knowledge graph with relationships
  - Cross-agent data dependencies and references
  - System-wide search index and metadata
  - Historical workflow results and performance metrics
- **Access Pattern**: High-frequency read/write from all agents

## âš™ï¸ System Coordination Components

### ğŸ›ï¸ Agent Orchestrator
**Primary Responsibilities**:
- **Lifecycle Management**: Start/stop agents, health monitoring
- **API Gateway**: Handle external requests, route to appropriate workflows  
- **Background Processing**: Manage async task queues and long-running operations
- **System Status**: Provide real-time status and performance metrics
- **Knowledge Coordination**: Facilitate cross-agent data sharing

### ğŸ”„ Workflow Engine
**Primary Responsibilities**:
- **Dependency Management**: Ensure proper agent execution order
- **Step Execution**: Parallel task execution where possible
- **Error Handling**: Retry logic, fallback strategies, graceful degradation
- **Progress Tracking**: Real-time workflow status and completion metrics
- **Result Aggregation**: Combine multi-agent outputs into cohesive results

### ğŸ“‹ Task Coordinator  
**Primary Responsibilities**:
- **Queue Management**: Priority-based task scheduling and load balancing
- **Task Routing**: Direct tasks to appropriate agents based on type and load
- **Status Tracking**: Monitor task progress, completion, and failure states
- **History Management**: Maintain audit trail of all task executions
- **Resource Allocation**: Manage API rate limits and computational resources

## ğŸ”„ Data Flow & Communication Patterns

### Sequential Pipeline (Primary Pattern)
```
Query â†’ Search â†’ Analysis â†’ Verification â†’ Content â†’ Response
   â†“        â†“         â†“           â†“          â†“        â†“
Memory   Memory    Memory      Memory     Memory   Knowledge
Store    Store     Store       Store      Store    Repository
```

### Cross-Agent Communication
- **Direct Memory Access**: Agents read from predecessor's memory
- **Shared Knowledge Updates**: Real-time updates to central repository  
- **Dependency Resolution**: Automatic data flow based on workflow dependencies
- **Error Propagation**: Failure states communicated upstream for retry logic

## ğŸš€ Key Architectural Benefits

### Performance & Scalability
- **Direct API Integration**: Zero-latency agent-to-API communication
- **Parallel Execution**: Independent agents can run simultaneously when dependencies allow
- **Memory Optimization**: Specialized storage reduces data duplication and improves access speed
- **Async Processing**: Non-blocking operations for better resource utilization

### Reliability & Fault Tolerance
- **Dependency Isolation**: Agent failures don't cascade through the system
- **Retry Mechanisms**: Automatic retry with exponential backoff for API failures
- **Graceful Degradation**: System continues operating with reduced functionality
- **Comprehensive Logging**: Full audit trail for debugging and monitoring

### Maintainability & Extensibility  
- **Modular Design**: Add/remove agents without affecting core system
- **Clear Separation**: Each component has well-defined responsibilities
- **API Abstraction**: Changes to external APIs isolated to individual agents
- **Configuration-Driven**: Processor selection and workflow customization via config

### Intelligence & Quality
- **Specialized Processing**: Each agent optimized for its specific task domain
- **Quality Assurance**: Multi-stage verification ensures high-quality outputs
- **Knowledge Accumulation**: System learns and improves from historical data
- **Contextual Awareness**: Agents have access to full workflow context for better decisions

## ğŸ”§ Technical Implementation Details

### API Integration Layer
```python
# All agents use the corrected Parallel.ai API pattern:
from parallel import AsyncParallel

client = AsyncParallel(api_key=settings.parallel_api_key)
result = await client.task_run.execute(
    input="Research query or analysis prompt",
    processor="base|lite|core|pro|ultra|ultra2x|ultra4x|ultra8x",
    output="Expected output description"
)
```

### Supported Processors
- **lite**: Fastest, basic processing
- **base**: Standard processing, good balance
- **core**: Enhanced processing with better accuracy  
- **pro**: Professional-grade processing for complex tasks
- **ultra**: Maximum processing power for complex analysis
- **ultra2x/4x/8x**: Scaled ultra processing for intensive workloads

### Workflow Configuration
```python
processor_config = {
    "search": "base",      # Fast web search
    "analysis": "core",    # Standard analysis
    "verification": "pro", # High-quality fact-checking
    "content": "lite"      # Fast content generation
}
```

### Error Handling Strategy
1. **API Failures**: Automatic retry with exponential backoff
2. **Agent Failures**: Graceful degradation with partial results
3. **Dependency Failures**: Skip dependent steps, continue with available data
4. **Timeout Handling**: Configurable timeouts per agent and overall workflow

## ğŸ“Š Performance Characteristics

### Typical Response Times
- **Search Agent**: 2-5 seconds (depends on processor)
- **Analyst Agent**: 3-8 seconds (depends on data complexity)
- **Critic Agent**: 4-10 seconds (depends on verification depth)
- **Writer Agent**: 2-6 seconds (depends on content length)
- **Total Pipeline**: 15-45 seconds for comprehensive research

### Resource Requirements
- **Memory**: ~100MB base + ~10MB per concurrent workflow
- **CPU**: Minimal (I/O bound operations)
- **Network**: High bandwidth recommended for API calls
- **Storage**: Configurable (SQLite default, PostgreSQL recommended for production)

## ğŸ›¡ï¸ Security & Privacy

### API Security
- **Authentication**: Secure API key management via environment variables
- **Rate Limiting**: Built-in respect for Parallel.ai rate limits
- **Request Validation**: Input sanitization and validation
- **Error Masking**: Sensitive information filtered from error responses

### Data Privacy
- **Memory Isolation**: Agent memories are logically separated
- **Data Retention**: Configurable retention policies for research data
- **Access Control**: Agent-level access controls for sensitive operations
- **Audit Logging**: Comprehensive logging for compliance and debugging

## ğŸš€ Deployment & Operations

### Development Setup
```bash
# Clone and setup
git clone <repository>
cd deep-research
pip install -r requirements.txt

# Configure environment
cp .env.example .env
# Edit .env with your API keys

# Run development server
python run.py
```

### Production Deployment
- **Web Server**: FastAPI with Uvicorn (async ASGI)
- **Database**: PostgreSQL recommended for production
- **Caching**: Redis for session and result caching
- **Monitoring**: Built-in health checks and metrics endpoints
- **Scaling**: Horizontal scaling via load balancer + multiple instances

### API Endpoints
- `POST /research` - Synchronous research pipeline
- `POST /research/async` - Asynchronous research with task ID
- `GET /task/{task_id}` - Check async task status
- `GET /status` - System health and status
- `GET /agents` - Agent status and performance metrics
- `GET /knowledge/search` - Search system knowledge
- `GET /docs` - Interactive API documentation

## ğŸ”® Future Enhancements

### Planned Features
- **Multi-Language Support**: Research in multiple languages
- **Custom Agent Types**: User-defined specialized agents
- **Advanced Analytics**: Machine learning insights on research patterns
- **Real-Time Collaboration**: Multi-user research sessions
- **Export Formats**: PDF, DOCX, and presentation formats

### Scalability Improvements
- **Distributed Processing**: Multi-node agent execution
- **Advanced Caching**: Intelligent result caching and reuse
- **Stream Processing**: Real-time result streaming
- **API Optimization**: Request batching and connection pooling

This architecture provides a robust, scalable, and intelligent foundation for automated research tasks while maintaining flexibility for future enhancements and customizations.
